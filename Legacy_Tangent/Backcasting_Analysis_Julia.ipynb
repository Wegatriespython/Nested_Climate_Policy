{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77815a21-95f3-45d0-8769-dfb505034397",
   "metadata": {},
   "source": [
    "# Backcasting Analysis for Transitions\n",
    "Backcasting is the science of taking a desired end goal and iteratively calculating the sequence of choices which lead to it. For example if we take the end goal as a bike friendly Prague, what decisions have to be made today to reach that end goal?\n",
    "\n",
    "### Why is this analysis so Quantitative ?\n",
    "Conventional scenario analysis is often centered around narratives which break up key uncertainities. This is a time consuming process and has some important limitations. One is that it can gets very complex for more than a few uncertainities as the number of possibilties grows explosively. Two it becomes hard to keep track of all the possible decisions.\n",
    "\n",
    "For example if we take a simplified version of this model for biking in Prague :\n",
    "-  Make decisions once in 4 years.\n",
    "-  6 Possible decisions or Policy Levers : Increasing Policy Coordination, Anti-Car Propaganda, Raise Prices Public Transport, Invest in Bike Infra, Lobby for Biking Finance, Levy Tax on Parking\n",
    "-  Each decision can have an effect on the share of biking. But decisions also interact with each other. For example increasing policy coordination will enhance the effectiveness of all policy measures. Anti-Car Propoganda can increase the chances of un-popular measures like parking taxes.\n",
    "\n",
    "Now if we take our end year as 2040. We have 3 sets of decisions to be made. 2^6 Possible decisions each period, for 3 periods that represents 262,144 possible transition paths. Now we have to find the ones which are successful.This is where a computational approach comes handy. We can use computers to check a vast proportion of those possible paths and precisely evaluate which ones are the best to a pre-defined criteria. In our case we have a utility function.\n",
    "\n",
    "It also becomes possible to specify the interactions amongst decisions and delayed effects of certain decision choices. Last we can use empirical data and expert judgement to estimate the key effects and probabilties involved.\n",
    "\n",
    "\n",
    "### How It Works\n",
    "1. Initial Conditions: We start with the current modal split (biking, cars, public transport, and others) and set future goals for biking share.\n",
    "2. Action Definition: We define a set of possible policy actions, each with specific effects, limitations, and interactions.\n",
    "Mean Field Approximation: This crucial component models the aggregate response of all other agents in the system (e.g., car manufacturers, oil companies, status quo supporters) to our policy actions and changing modal shares.\n",
    "State Transition Exploration: The script systematically explores all possible combinations of actions over time, creating a comprehensive map of potential futures.\n",
    "Path Identification: It identifies paths that reach the desired biking share goals, considering both direct effects of actions and the Mean Field response.\n",
    "6. Optimization: The script determines the most efficient path - the one that reaches the goal with the fewest actions, considering both effectiveness and societal pushback.\n",
    "Visualization: It generates a graph showing the evolution of biking share over time along the optimal path, including the timing of policy actions.\n",
    "\n",
    "### User Inputs and Their Effects\n",
    "You can change various inputs to explore different scenarios:\n",
    "\n",
    "TIME_PERIODS: Adjust the number of time steps in your analysis.\n",
    "\n",
    "initial_modal_shares: Change the starting percentages for each transportation mode.\n",
    "\n",
    "final_states: Modify the target biking share percentages. We have used 4% as the baseline, with 7% being realistic and 20% as the dream.\n",
    "\n",
    "ACTIONS: You can define the full set of actions that the city can take. Note however this dramatically increases the model complexity and it might take much longer to find a solution.\n",
    "For each action, you can adjust:\n",
    "- effects: How much it changes each mode's share. For example raising public transport fares might increase car and bike shares.\n",
    "- cooldown: How long before the action can be used again.\n",
    "- one_time: Whether the action can only be used once.\n",
    "- success_probability: The chance the action will work.\n",
    "- interactions: How this action affects the success of other actions.\n",
    "- costs: Each action can have an associated cost, for example investing in biking infrastructure is expensive and might not pay off.\n",
    "- MEAN_FIELD_EFFECTS: This calculates the aggregate reaction of all other agents in the system to our policy response. In our case they represent the status quo who want to retain the current car centric modal share.\n",
    "\n",
    "- utility_function: Change how the \"goodness\" of a particular modal split is calculated.Both our city and our opposition(MEAN_FEILD_EFFECTS) value the modal shift differently and have different costs for their respective actions.\n",
    "\n",
    "- EXOGENOUS_FACTORS: Add external influences that happen at specific times. Not yet implemented but factors like E-bikes, EU policy, weather, Pandemics affect the state transtitions and can be modelled by their probabiliy distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06084208-5948-4179-a5c9-4f6aeadf488a",
   "metadata": {},
   "source": [
    "### Install all required packages to the IJulia Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36192830-d1fc-4ec3-acc7-8b53f27e143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using StatsBase\n",
    "using DataStructures\n",
    "using Plots\n",
    "using Random\n",
    "using Combinatorics\n",
    "using Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59766a97-9bcd-4b6d-8bea-8cd4e40f3ccb",
   "metadata": {},
   "source": [
    "### User Input Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5463739-24f0-4aca-9daa-eed9952db2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant Main.TIME_PERIODS. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.DECISIONS. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.DECISION_INDICES. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.initial_modal_shares. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.final_states. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.ACTIONS. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.MEAN_FIELD_EFFECTS. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.EXOGENOUS_FACTORS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mean_field_response (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time periods\n",
    "const TIME_PERIODS = [1, 2, 3, 4]\n",
    "\n",
    "# Define actions and their indices\n",
    "const DECISIONS = [\"IPC\", \"ACP\", \"RPPT\", \"IBI\", \"LBF\", \"LTP\"]\n",
    "const DECISION_INDICES = Dict(name => idx for (idx, name) in enumerate(DECISIONS))\n",
    "\n",
    "# Initial modal shares (should sum to 1)\n",
    "const initial_modal_shares = Dict(\"s\" => 0.01, \"c\" => 0.50, \"p\" => 0.40, \"o\" => 0.09)\n",
    "\n",
    "# Desired final biking modal shares\n",
    "const final_states = [0.15, 0.10, 0.07]\n",
    "\n",
    "# Action definitions\n",
    "struct Action\n",
    "    name::String\n",
    "    effects::Dict{String, Float64}\n",
    "    cooldown::Int\n",
    "    one_time::Bool\n",
    "    success_probability::Float64\n",
    "    interactions::Dict{String, Float64}\n",
    "    costs::Dict{String, Float64}\n",
    "end\n",
    "\n",
    "function Action(name; effects=Dict{String, Float64}(), cooldown=0, one_time=false, success_probability=1.0, interactions=Dict{String, Float64}(), costs=Dict{String, Float64}())\n",
    "    effects = _normalize_effects(effects)\n",
    "    return Action(name, effects, cooldown, one_time, success_probability, interactions, costs)\n",
    "end\n",
    "\n",
    "function _normalize_effects(effects::Dict{String, Float64})\n",
    "    total_effect = sum(values(effects))\n",
    "    if total_effect != 0\n",
    "        unspecified_modes = setdiff(Set([\"s\", \"c\", \"p\", \"o\"]), Set(keys(effects)))\n",
    "        adjustment = -total_effect / length(unspecified_modes)\n",
    "        for mode in unspecified_modes\n",
    "            effects[mode] = adjustment\n",
    "        end\n",
    "    end\n",
    "    return effects\n",
    "end\n",
    "\n",
    "function evaluate_success_probability(action::Action, executed_actions)\n",
    "    probability = action.success_probability\n",
    "    for (interaction_action, adjustment) in action.interactions\n",
    "        if get(executed_actions, interaction_action, 0) == 1\n",
    "            probability += adjustment\n",
    "        end\n",
    "    end\n",
    "    return clamp(probability, 0.0, 1.0)\n",
    "end\n",
    "\n",
    "function calculate_cost(action::Action, executed_actions)\n",
    "    return action.costs[\"base\"]\n",
    "end\n",
    "\n",
    "# Define the actions with their parameters\n",
    "const ACTIONS = Dict(\n",
    "    \"IPC\" => Action(\"IPC\", cooldown=1, costs=Dict(\"base\" => 0.01)),\n",
    "    \"ACP\" => Action(\"ACP\", effects=Dict(\"s\" => 0.01, \"c\" => -0.005), cooldown=1, costs=Dict(\"base\" => 0.01)),\n",
    "    \"RPPT\" => Action(\"RPPT\", effects=Dict(\"p\" => -0.005, \"s\" => 0.0025, \"c\" => 0.0025), one_time=true, success_probability=0.2, interactions=Dict(\"IPC\" => 0.2, \"LBF\" => 0.2), costs=Dict(\"base\" => 0.1)),\n",
    "    \"IBI\" => Action(\"IBI\", effects=Dict(\"s\" => 0.02, \"c\" => -0.01), one_time=true, success_probability=0.2, interactions=Dict(\"IPC\" => 0.2, \"LBF\" => 0.2), costs=Dict(\"base\" => 0.1)),\n",
    "    \"LBF\" => Action(\"LBF\", cooldown=1, success_probability=0.2, interactions=Dict(\"ACP\" => 0.2), costs=Dict(\"base\" => 0.01)),\n",
    "    \"LTP\" => Action(\"LTP\", effects=Dict(\"c\" => -0.007, \"s\" => 0.0035, \"p\" => 0.0035), one_time=true, success_probability=0.2, interactions=Dict(\"ACP\" => 0.2, \"IPC\" => 0.2), costs=Dict(\"base\" => 0.01))\n",
    ")\n",
    "\n",
    "const MEAN_FIELD_EFFECTS = Dict(\n",
    "    \"base\" => Dict(\"effects\" => Dict(\"c\" => 0.000, \"p\" => 0.000), \"probability\" => 0.8),\n",
    "    \"reactions\" => Dict(\n",
    "        \"ACP\" => Dict(\"effects\" => Dict(\"c\" => 0.002), \"probability\" => 0.7),\n",
    "        \"LTP\" => Dict(\"effects\" => Dict(\"c\" => 0.002, \"p\" => 0.002), \"probability\" => 0.6),\n",
    "        \"RPPT\" => Dict(\"effects\" => Dict(\"c\" => 0.0025, \"s\" => -0.0025), \"probability\" => 0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Utility function (with normalized bike share gain)\n",
    "function utility_function(path)\n",
    "    if isempty(path)\n",
    "        return -Inf  # Return negative infinity for empty paths\n",
    "    end\n",
    "    \n",
    "    target_biking_share = minimum(final_states)\n",
    "    initial_biking_share = initial_modal_shares[\"s\"]\n",
    "    max_possible_gain = target_biking_share - initial_biking_share\n",
    "    \n",
    "    total_utility = 0.0\n",
    "    total_cost = 0.0\n",
    "    \n",
    "    for (i, step) in enumerate(path)\n",
    "        modal_shares = step[\"modal_shares\"]\n",
    "        bike_share_gain = modal_shares[\"s\"] - initial_biking_share\n",
    "        normalized_gain = bike_share_gain / max_possible_gain\n",
    "        step_utility = normalized_gain\n",
    "        total_utility += step_utility\n",
    "\n",
    "        # Calculate cost for this step\n",
    "        step_cost = sum(get(ACTIONS[DECISIONS[i]].costs, \"base\", 0.0) for (i, d) in enumerate(step[\"decisions\"]) if d == 1; init=0.0)\n",
    "        total_cost += step_cost\n",
    "    end\n",
    "    \n",
    "    # Normalize cost to be between 0 and 1\n",
    "    max_possible_cost = sum(action.costs[\"base\"] for action in values(ACTIONS)) * length(path)\n",
    "    normalized_cost = total_cost / max_possible_cost\n",
    "    \n",
    "    # Combine normalized utility and cost (you can adjust the weights if needed)\n",
    "    return 0.7 * total_utility - 0.3 * normalized_cost\n",
    "end\n",
    "\n",
    "# Exogenous factors (if any)\n",
    "const EXOGENOUS_FACTORS = Dict(\n",
    "    1 => Dict(\"effects\" => Dict(\"s\" => 0.01, \"c\" => -0.005, \"p\" => -0.0025, \"o\" => -0.0025), \"probability\" => 0.7, \"label\" => \"E-Bikes\"),\n",
    "    2 => Dict(\"effects\" => Dict(\"s\" => 0.01, \"c\" => -0.005, \"p\" => -0.0025, \"o\" => -0.0025), \"probability\" => 0.01, \"label\" => \"Pandemic\"),\n",
    "    3 => Dict(\"effects\" => Dict(\"s\" => 0.05, \"c\" => -0.025, \"p\" => -0.0125, \"o\" => -0.0125), \"probability\" => 0.05, \"label\" => \"EU Biking Directive\"),\n",
    "    4 => Dict(\"effects\" => Dict(\"s\" => 0.01, \"p\" => -0.002, \"c\" => -0.004, \"o\" => -0.004), \"probability\" => 0.8, \"label\" => \"Shared Biking\"),\n",
    "    5 => Dict(\"effects\" => Dict(\"s\" => 0.5, \"p\" => 0.3, \"c\" => -0.4, \"o\" => -0.4), \"probability\" => 1e-4, \"label\" => \"Tipping point Social Perception\")\n",
    ")\n",
    "\n",
    "# Add this function before the state_transition function\n",
    "\n",
    "function mean_field_response(modal_shares, decisions)\n",
    "    mf_effects = Dict(\"s\" => 0.0, \"c\" => 0.0, \"p\" => 0.0, \"o\" => 0.0)\n",
    "    \n",
    "    # Apply base effects\n",
    "    if rand() < MEAN_FIELD_EFFECTS[\"base\"][\"probability\"]\n",
    "        for (mode, delta) in MEAN_FIELD_EFFECTS[\"base\"][\"effects\"]\n",
    "            mf_effects[mode] += delta\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Apply reaction effects based on our decisions\n",
    "    for (action_name, reaction) in MEAN_FIELD_EFFECTS[\"reactions\"]\n",
    "        idx = DECISION_INDICES[action_name]\n",
    "        if decisions[idx] == 1\n",
    "            if rand() < reaction[\"probability\"]\n",
    "                for (mode, delta) in reaction[\"effects\"]\n",
    "                    mf_effects[mode] += delta\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return mf_effects\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dda5c-25eb-4df4-8a4f-e731c51adf5d",
   "metadata": {},
   "source": [
    "### Mathematical Formulation of the Backcasting Analysis Model\n",
    "\n",
    "#### 1. Indices and Sets\n",
    "\n",
    "- Time periods: $t \\in T = \\{0, 1, 2, ..., T_{max}\\}$\n",
    "- Actions: $a \\in A = \\{\\text{IPC, ACP, RPPT, IBI, LBF, LTP}\\}$\n",
    "   - IPC : Incease Policy Coordination \n",
    "   - ACP : Anti-Car Propoganda\n",
    "   - RPPT : Raise Prices Public Transport \n",
    "   - IBI : Invest in Biking Infrastructure\n",
    "   - LBF : Lobby for Biking Finance \n",
    "   - LTP : Levy Taxes on Car Parking \n",
    "- Transportation modes: $m \\in M = \\{s, c, p, o\\}$ (biking, car, public transport, other)\n",
    "\n",
    "#### 2. State Variables\n",
    "\n",
    "- Modal shares: $x_{m,t} \\in [0,1]$, where $\\sum_{m \\in M} x_{m,t} = 1, \\forall t$\n",
    "- Action availability: $A_{a,t} \\in \\{0,1\\}$\n",
    "- Cooldown timers: $C_{a,t} \\in \\mathbb{N}_0$\n",
    "- Disabled actions: $D_t \\subseteq A$\n",
    "\n",
    "#### 3. Decision Variables\n",
    "\n",
    "- Action execution: $y_{a,t} \\in \\{0,1\\}$, subject to $y_{a,t} \\leq A_{a,t}$\n",
    "\n",
    "#### 4. Parameters\n",
    "\n",
    "- Initial modal shares: $x_{m,0}$\n",
    "- Target modal shares: $S_{target} = \\{s_1, s_2, s_3\\}$\n",
    "- Action effects: $\\delta_{a,m}$\n",
    "- Action cooldowns: $c_a$\n",
    "- Action one-time flags: $o_a \\in \\{0,1\\}$\n",
    "- Action base success probabilities: $p_a$\n",
    "- Action interaction effects: $\\gamma_{a,a'}$\n",
    "- Action costs: $k_a$\n",
    "- Mean field base effects: $\\mu_{base,m}$\n",
    "- Mean field reaction effects: $\\mu_{a,m}$\n",
    "- Exogenous factors: $E_{t,m}$ with probabilities $p_{E_t}$\n",
    "\n",
    "#### 5. State Transition Functions\n",
    "\n",
    "a. Modal Share Update:\n",
    "\n",
    "$$x_{m,t+1} = x_{m,t} + \\sum_{a \\in A} (\\eta_{a,t} \\cdot y_{a,t} \\cdot \\delta_{a,m}) + \\text{MF}_t + E_t$$\n",
    "\n",
    "where:\n",
    "- $\\eta_{a,t}$ is a Bernoulli random variable with $P(\\eta_{a,t} = 1) = \\min(1, \\max(0, p_a + \\sum_{a' \\in A} y_{a',t} \\cdot \\gamma_{a,a'}))$\n",
    "- $\\text{MF}_t$ represents the mean field effects\n",
    "- $E_t$ represents the exogenous factors\n",
    "\n",
    "b. Action Availability Update:\n",
    "\n",
    "$$A_{a,t+1} = \\begin{cases}\n",
    "0 & \\text{if } C_{a,t+1} > 0 \\text{ or } a \\in D_{t+1} \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "c. Cooldown Timer Update:\n",
    "\n",
    "$$C_{a,t+1} = \\begin{cases}\n",
    "c_a & \\text{if } y_{a,t} = 1 \\\\\n",
    "\\max(0, C_{a,t} - 1) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "d. Disabled Actions Update:\n",
    "\n",
    "$$D_{t+1} = D_t \\cup \\{a \\in A : o_a = 1 \\text{ and } y_{a,t} = 1\\}$$\n",
    "\n",
    "#### 6. Mean Field Approximation\n",
    "\n",
    "$$\\text{MF}_t = \\xi_{base,t} \\cdot \\mu_{base,m} + \\sum_{a \\in A} y_{a,t} \\cdot \\mu_{a,m} \\cdot \\xi_{a,t}$$\n",
    "\n",
    "where $\\xi_{base,t}$ and $\\xi_{a,t}$ are Bernoulli random variables with probabilities defined in the mean field effects.\n",
    "\n",
    "#### 7. Utility Function\n",
    "\n",
    "For a path $p = (x_t, y_t)_{t=0}^{T_{max}}$:\n",
    "\n",
    "$$U(p) = 0.7 \\cdot \\sum_{t=1}^{T_{max}} \\frac{x_{s,t} - x_{s,0}}{s_{target} - x_{s,0}} - 0.3 \\cdot \\frac{\\sum_{t=1}^{T_{max}} \\sum_{a \\in A} y_{a,t} \\cdot k_a}{K_{max}}$$\n",
    "\n",
    "where $s_{target} = \\min(S_{target})$ and $K_{max}$ is the maximum possible total cost.\n",
    "\n",
    "#### 8. Optimization Problem\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\max_{y_{a,t}} & \\quad \\mathbb{E}[U(p)] \\\\\n",
    "\\text{s.t.} & \\quad |x_{s,T_{max}} - s| < \\epsilon \\text{ for some } s \\in S_{target} \\\\\n",
    "& \\quad y_{a,t} \\leq A_{a,t} \\quad \\forall a \\in A, t \\in T \\\\\n",
    "& \\quad \\sum_{m \\in M} x_{m,t} = 1 \\quad \\forall t \\in T \\\\\n",
    "& \\quad 0 \\leq x_{m,t} \\leq 1 \\quad \\forall m \\in M, t \\in T\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### 9. Solution Approach\n",
    "\n",
    "1. Graph Building and Path Finding:\n",
    "   For each Monte Carlo simulation:\n",
    "   \n",
    "   a. Initialize the root node $v_0$ with:\n",
    "      - Time $t_0 = 0$\n",
    "      - Modal shares $x_{m,0}$\n",
    "      - Empty decision set\n",
    "      - Zero cost\n",
    "      - All actions available and no cooldowns\n",
    "\n",
    "   b. Create a queue $Q$ and enqueue $v_0$\n",
    "\n",
    "   c. While $Q$ is not empty:\n",
    "      1.   Dequeue a node $v$\n",
    "      2.  If $t_v = T_{max}$:\n",
    "           - If $|x_{s,v} - s| < \\epsilon$ for some $s \\in S_{target}$, add the path to $v$ to the set of valid paths $P$\n",
    "           - Continue to the next node in $Q$\n",
    "      3. Generate all possible action combinations $\\mathcal{C} = \\{c \\subseteq A : A_{a,v} = 1 \\text{ for all } a \\in c\\}$\n",
    "      4.  For each combination $c \\in \\mathcal{C}$:\n",
    "           - Create a new decision vector $y$ where $y_a = 1$ if $a \\in c$, else $y_a = 0$\n",
    "           - Apply the state transition function to get new modal shares $x'$, cooldowns $C'$, and disabled actions $D'$\n",
    "           - Calculate the cost of this transition\n",
    "           - Create a new node $v'$ with updated state variables\n",
    "           - Add an edge from $v$ to $v'$\n",
    "           - Enqueue $v'$ to $Q$\n",
    "\n",
    "   d. From the set of valid paths $P$, select the top $k$ paths based on the utility function $U(p)$\n",
    "\n",
    "2. Monte Carlo Simulation:\n",
    "   - Perform $N$ iterations of the graph building and path finding process\n",
    "   - Collect all top $k$ paths from each iteration into a set $P_{all}$\n",
    "\n",
    "3. Best Path Selection:\n",
    "   $$p^* = \\underset{p \\in P_{all}}{\\text{argmax }} U(p)$$\n",
    "\n",
    "#### 10. Complexity Analysis\n",
    "\n",
    "- Graph building complexity per simulation: $O(|T| \\cdot 2^{|A|})$\n",
    "  - At each time step, we consider all possible action combinations\n",
    "  - The number of nodes at each level can grow exponentially with the number of actions\n",
    "- Path selection complexity per simulation: $O(|P| \\cdot |T|)$, where $|P|$ is the number of valid paths\n",
    "- Overall time complexity: $O(N \\cdot (|T| \\cdot 2^{|A|} + |P| \\cdot |T|))$\n",
    "\n",
    "where $N$ is the number of Monte Carlo simulations.\n",
    "\n",
    "Note: In practice, the actual runtime may be lower due to pruning of infeasible paths and early termination when target states are reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1577900-c239-4f67-ad82-8a839ddec575",
   "metadata": {},
   "source": [
    "### Core Simulation Logic\n",
    "\n",
    "1. **State Representation**:\n",
    "   The simulation uses a `StateNode` struct to represent each state. This includes:\n",
    "   - The current time period\n",
    "   - Modal shares for different transportation modes (biking, car, public transport, other)\n",
    "   - Decisions made to reach this state\n",
    "   - Parent node reference\n",
    "   - Children nodes\n",
    "   - Accumulated cost\n",
    "   - Debug log\n",
    "   - Cooldowns for actions\n",
    "   - Set of disabled actions\n",
    "\n",
    "2. **Graph Building**:\n",
    "   The `build_graph` function constructs a tree of possible states:\n",
    "   - It starts with a root node representing the initial modal shares.\n",
    "   - For each time step, it generates all possible combinations of available actions.\n",
    "   - It applies these actions to the current state using the `state_transition` function.\n",
    "   - New states are created as child nodes, considering action cooldowns and disabled actions.\n",
    "   - This process continues until reaching the final time period.\n",
    "\n",
    "3. **State Transition**:\n",
    "   The `state_transition` function simulates the effects of chosen actions:\n",
    "   - It calculates the changes in modal shares based on the selected actions.\n",
    "   - It considers the success probability of each action.\n",
    "   - It applies exogenous effects to simulate external factors.\n",
    "   - It updates cooldowns for actions and disables one-time actions.\n",
    "\n",
    "4. **Path Collection**:\n",
    "   As the graph is built, paths that reach states close to the desired final modal shares are collected:\n",
    "   - A path is considered valid if its final state is within a small tolerance of any target final state.\n",
    "   - Each valid path is stored as a sequence of states from the initial to the final state.\n",
    "\n",
    "5. **Path Evaluation**:\n",
    "   The `evaluate_path` function assesses the quality of each collected path:\n",
    "   - It uses a utility function to score the path based on its final state and possibly other factors like cost.\n",
    "\n",
    "6. **Best Path Selection**:\n",
    "   The `find_best_path` function selects the optimal path from all collected paths:\n",
    "   - It compares the evaluation scores of all paths.\n",
    "   - The path with the highest score is chosen as the best path.\n",
    "\n",
    "7. **Result Visualization**:\n",
    "   The `visualize_best_path` function creates a plot of the best path:\n",
    "   - It shows the biking modal share over time.\n",
    "   - It annotates the plot with successful and failed actions at each time step.\n",
    "\n",
    "8. **Output Generation**:\n",
    "   The simulation produces several outputs:\n",
    "   - A detailed description of the best path, including decisions made at each step and resulting modal shares.\n",
    "   - A visual plot of the best path.\n",
    "   - CSV files containing data on the best paths and analysis results.\n",
    "\n",
    "This simulation approach explores all possible decision sequences within the constraints of the model, allowing for the identification of optimal strategies to increase biking modal share. It accounts for the complexities of action interactions, cooldowns, and stochastic effects in the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd45b1a-1aeb-48ca-8c2a-7dcd7103b9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visualize_best_path (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify the state_transition function\n",
    "function state_transition(modal_shares_prev, decisions, current_time)\n",
    "    modal_shares = copy(modal_shares_prev)\n",
    "    executed_actions = Dict(action_name => decisions[DECISION_INDICES[action_name]] for action_name in DECISIONS)\n",
    "    actions_to_disable = Set()\n",
    "    total_cost = 0.0\n",
    "    debug_log = String[]\n",
    "\n",
    "    for (action_name, action) in ACTIONS\n",
    "        if get(executed_actions, action_name, 0) == 1\n",
    "            success_prob = evaluate_success_probability(action, executed_actions)\n",
    "            success = rand() <= success_prob\n",
    "            if success\n",
    "                push!(debug_log, \"Action $action_name succeeded:\")\n",
    "                for (mode, delta) in action.effects\n",
    "                    modal_shares[mode] += delta\n",
    "                    push!(debug_log, \"  $mode: $(round(delta, digits=4))\")\n",
    "                end\n",
    "                total_cost += calculate_cost(action, executed_actions)\n",
    "            else\n",
    "                push!(debug_log, \"Action $action_name failed\")\n",
    "                if action.one_time\n",
    "                    push!(actions_to_disable, action_name)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    mf_effects = mean_field_response(modal_shares_prev, decisions)\n",
    "    push!(debug_log, \"Mean Field Effects:\")\n",
    "    for (mode, delta) in mf_effects\n",
    "        modal_shares[mode] += delta\n",
    "        push!(debug_log, \"  $mode: $(round(delta, digits=4))\")\n",
    "    end\n",
    "\n",
    "    # Exogenous factors\n",
    "    exogenous_effects = get(EXOGENOUS_FACTORS, current_time, Dict())\n",
    "    if !isempty(exogenous_effects)\n",
    "        push!(debug_log, \"Exogenous Effects ($(exogenous_effects[\"label\"])):\")\n",
    "        if rand() < exogenous_effects[\"probability\"]\n",
    "            total_exogenous_effect = sum(values(exogenous_effects[\"effects\"]))\n",
    "            adjustment = -total_exogenous_effect / (length(modal_shares) - length(exogenous_effects[\"effects\"]))\n",
    "            for mode in keys(modal_shares)\n",
    "                delta = get(exogenous_effects[\"effects\"], mode, adjustment)\n",
    "                modal_shares[mode] += delta\n",
    "                push!(debug_log, \"  $mode: $(round(delta, digits=4))\")\n",
    "            end\n",
    "        else\n",
    "            push!(debug_log, \"  No $(exogenous_effects[\"label\"]) effects applied (probability not met)\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Ensure modal shares are within [0,1]\n",
    "    for mode in keys(modal_shares)\n",
    "        if modal_shares[mode] < 0 || modal_shares[mode] > 1\n",
    "            push!(debug_log, \"Clipping $mode from $(round(modal_shares[mode], digits=4)) to $(round(clamp(modal_shares[mode], 0, 1), digits=4))\")\n",
    "        end\n",
    "        modal_shares[mode] = clamp(modal_shares[mode], 0, 1)\n",
    "    end\n",
    "\n",
    "    # Normalize modal shares to sum to 1\n",
    "    total_share = sum(values(modal_shares))\n",
    "    if !isapprox(total_share, 1.0, atol=1e-6)\n",
    "        push!(debug_log, \"Adjusting modal shares to sum to 1:\")\n",
    "        adjustment_factor = 1 / total_share\n",
    "        for mode in keys(modal_shares)\n",
    "            old_share = modal_shares[mode]\n",
    "            modal_shares[mode] *= adjustment_factor\n",
    "            push!(debug_log, \"  $mode: $(round(old_share, digits=4)) -> $(round(modal_shares[mode], digits=4))\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return modal_shares, actions_to_disable, total_cost, debug_log\n",
    "end\n",
    "\n",
    "function set_random_seed(seed=nothing)\n",
    "    if seed !== nothing\n",
    "        Random.seed!(seed)\n",
    "    end\n",
    "end\n",
    "# State node representation\n",
    "mutable struct StateNode\n",
    "    time::Int\n",
    "    modal_shares::Dict{String, Float64}\n",
    "    decisions::Union{Nothing, Vector{Int}}\n",
    "    parent::Union{Nothing, StateNode}\n",
    "    children::Vector{StateNode}\n",
    "    cost::Float64\n",
    "    debug_log::Vector{String}\n",
    "    cooldowns::Dict{String, Int}\n",
    "    disabled_actions::Set{String}\n",
    "end\n",
    "\n",
    "function StateNode(time, modal_shares; decisions=nothing, parent=nothing, cooldowns=nothing, disabled_actions=nothing, cost=0.0, debug_log=String[])\n",
    "    if cooldowns === nothing\n",
    "        cooldowns = Dict(action => 0 for action in DECISIONS)\n",
    "    else\n",
    "        cooldowns = copy(cooldowns)\n",
    "    end\n",
    "    if disabled_actions === nothing\n",
    "        disabled_actions = Set{String}()\n",
    "    else\n",
    "        disabled_actions = copy(disabled_actions)\n",
    "    end\n",
    "    return StateNode(time, copy(modal_shares), decisions, parent, StateNode[], cost, debug_log, cooldowns, disabled_actions)\n",
    "end\n",
    "\n",
    "function build_graph(initial_modal_shares, final_states)\n",
    "    root = StateNode(0, initial_modal_shares)\n",
    "    queue = Deque{StateNode}()\n",
    "    push!(queue, root)  # Use push! to add the root node to the queue\n",
    "    paths = []\n",
    "\n",
    "    while !isempty(queue)\n",
    "        current_node = popfirst!(queue)\n",
    "        if current_node.time == length(TIME_PERIODS)\n",
    "            if any(isapprox(current_node.modal_shares[\"s\"], final_state, atol=0.005) for final_state in final_states)\n",
    "                path = []\n",
    "                node = current_node\n",
    "                while node.parent !== nothing\n",
    "                    push!(path, Dict(\n",
    "                        \"time\" => node.time,\n",
    "                        \"decisions\" => node.decisions,\n",
    "                        \"modal_shares\" => node.modal_shares,\n",
    "                        \"cost\" => node.cost,\n",
    "                        \"debug_log\" => node.debug_log\n",
    "                    ))\n",
    "                    node = node.parent\n",
    "                end\n",
    "                reverse!(path)\n",
    "                push!(paths, path)\n",
    "            end\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        # Decrease cooldowns\n",
    "        cooldowns_next = Dict(action => max(current_node.cooldowns[action] - 1, 0) for action in DECISIONS)\n",
    "\n",
    "        # Determine available actions\n",
    "        available_actions = [action for action in DECISIONS if cooldowns_next[action] == 0 && !(action in current_node.disabled_actions)]\n",
    "\n",
    "        # Generate all possible combinations of available actions (power set)\n",
    "        action_combinations = [collect(comb) for r in 0:length(available_actions) for comb in combinations(available_actions, r)]\n",
    "\n",
    "        # For each combination, create the decision vector\n",
    "        for action_subset in action_combinations\n",
    "            decisions = zeros(Int, length(DECISIONS))\n",
    "            for action in action_subset\n",
    "                idx = DECISION_INDICES[action]\n",
    "                decisions[idx] = 1\n",
    "            end\n",
    "\n",
    "            modal_shares_next, actions_to_disable, step_cost, debug_log = state_transition(\n",
    "                current_node.modal_shares,\n",
    "                decisions,\n",
    "                current_node.time\n",
    "            )\n",
    "\n",
    "            # Update cooldowns and disabled actions\n",
    "            cooldowns_updated = copy(cooldowns_next)\n",
    "            disabled_actions_updated = copy(current_node.disabled_actions)\n",
    "\n",
    "            # Set cooldowns and disable actions as needed\n",
    "            for action in action_subset\n",
    "                action_obj = ACTIONS[action]\n",
    "                idx = DECISION_INDICES[action]\n",
    "                # Set cooldown if action has a cooldown\n",
    "                if action_obj.cooldown !== nothing\n",
    "                    cooldowns_updated[action] = action_obj.cooldown\n",
    "                end\n",
    "                # Disable one-time actions\n",
    "                if action_obj.one_time\n",
    "                    push!(disabled_actions_updated, action)\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # Disable actions that failed\n",
    "            union!(disabled_actions_updated, actions_to_disable)\n",
    "\n",
    "            # Create child node\n",
    "            child_node = StateNode(\n",
    "                current_node.time + 1,\n",
    "                modal_shares_next,\n",
    "                decisions=decisions,\n",
    "                parent=current_node,\n",
    "                cooldowns=cooldowns_updated,\n",
    "                disabled_actions=disabled_actions_updated,\n",
    "                cost=current_node.cost + step_cost,\n",
    "                debug_log=debug_log\n",
    "            )\n",
    "            push!(current_node.children, child_node)\n",
    "            push!(queue, child_node)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return paths\n",
    "end\n",
    "\n",
    "function evaluate_path(path)\n",
    "    return utility_function(path)\n",
    "end\n",
    "\n",
    "function find_best_path(paths)\n",
    "    if isempty(paths)\n",
    "        return nothing\n",
    "    end\n",
    "    return argmax(evaluate_path, paths)\n",
    "end\n",
    "\n",
    "function visualize_best_path(path)\n",
    "    times = [0; [step[\"time\"] for step in path]]\n",
    "    shares = [initial_modal_shares[\"s\"]; [step[\"modal_shares\"][\"s\"] for step in path]]\n",
    "    \n",
    "    # Configure plot with adjusted margins and larger size\n",
    "    p = plot(times, shares, marker=:circle,\n",
    "             xlabel=\"Time Period\", ylabel=\"Biking Modal Share\", \n",
    "             title=\"Best Path for Increasing Biking Modal Share\",\n",
    "             label=\"Biking Modal Share Path\", linewidth=2, \n",
    "             size=(1000, 800), left_margin=15mm, right_margin=20mm, top_margin=15mm, bottom_margin=15mm)\n",
    "\n",
    "    # Loop through each step to add annotations\n",
    "    for (i, step) in enumerate(path)\n",
    "        y_pos = step[\"modal_shares\"][\"s\"]\n",
    "        \n",
    "        annotations = String[]\n",
    "        successful_actions = String[]\n",
    "        failed_actions = String[]\n",
    "        exogenous_effect = nothing\n",
    "        \n",
    "        for log_entry in step[\"debug_log\"]\n",
    "            if contains(log_entry, \"succeeded\")\n",
    "                action = split(log_entry)[2]\n",
    "                push!(successful_actions, action)\n",
    "            elseif contains(log_entry, \"failed\")\n",
    "                action = split(log_entry)[2]\n",
    "                push!(failed_actions, action)\n",
    "            elseif startswith(log_entry, \"Exogenous Effects\")\n",
    "                exogenous_effect = split(log_entry, \":\")[1]\n",
    "            elseif contains(log_entry, \"s:\") && !isnothing(exogenous_effect)\n",
    "                push!(annotations, exogenous_effect)\n",
    "                exogenous_effect = nothing\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if !isempty(successful_actions)\n",
    "            push!(annotations, \"Successful: $(join(successful_actions, \", \"))\")\n",
    "        end\n",
    "        if !isempty(failed_actions)\n",
    "            push!(annotations, \"Failed: $(join(failed_actions, \", \"))\")\n",
    "        end\n",
    "        \n",
    "        annotation_text = join(annotations, \"\\n\")\n",
    "        \n",
    "        # Add annotation only if it exists, adjust position to avoid overlap\n",
    "        if !isempty(annotation_text)\n",
    "            annotate!(p, step[\"time\"], y_pos + 0.005, text(annotation_text, 6))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Adding descriptive legend items for each action type\n",
    "    legend_text = [\"IPC: Improve Policy Coordination\",\n",
    "                   \"ACP: Anti-Car Propaganda\",\n",
    "                   \"RPPT: Raise Prices Public Transport\",\n",
    "                   \"IBI: Invest in Bike Infrastructure\",\n",
    "                   \"LBF: Lobby for Biking Finance\",\n",
    "                   \"LTP: Levy Tax on Car Parking\"]\n",
    "\n",
    "    # Adding dummy series for the legend\n",
    "    for label in legend_text\n",
    "        plot!([NaN], [NaN], label=label, linestyle=:solid, marker=:none)\n",
    "    end\n",
    "\n",
    "    # Move legend to an outer position to avoid crowding\n",
    "    plot!(p, legend=:outertopright)\n",
    "\n",
    "    # Display and save the updated plot\n",
    "    display(p)\n",
    "    savefig(p, \"best_path_plot.png\")\n",
    "    println(\"Plot saved as 'best_path_plot.png'\")\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c46da7-c62a-4c2f-b919-479deeabca5d",
   "metadata": {},
   "source": [
    "### Monte Carlo Analysis\n",
    "\n",
    "1. **Monte Carlo Simulation Process**:\n",
    "   The `monte_carlo_analysis` function is implemented in the `Backcasting_run.jl` file. It works as follows:\n",
    "   - The function takes a parameter `num_simulations` (set to 300 in the main function).\n",
    "   - It runs the `run_single_simulation` function multiple times (300 in this case).\n",
    "   - Each run of `run_single_simulation` builds a new graph and finds the top 10 best paths.\n",
    "   - All these best paths from each simulation are collected into a single list.\n",
    "\n",
    "2. **Analysis of Results**:\n",
    "   The collected best paths are analyzed for:\n",
    "   - **Action frequencies**\n",
    "   - **Cost distribution** (average, max, and min costs)\n",
    "   - **Risk categorization** (high-risk vs. low-risk paths based on cost)\n",
    "   - Results are saved to CSV files for further analysis.\n",
    "\n",
    "3. **Best Path Selection**:\n",
    "   - From all the collected paths across all simulations, the single best path is selected.\n",
    "   - This path is then visualized and its details are printed.\n",
    "\n",
    "4. **Reasons for Using Monte Carlo Analysis**:\n",
    "   - **Handling Stochasticity**:\n",
    "     - The simulation includes several stochastic elements, such as action success probabilities, exogenous effects on modal shares, and mean-field approximations of other players' actions.\n",
    "     - Monte Carlo analysis allows us to explore how these random factors affect outcomes over many iterations.\n",
    "   - **Robustness of Strategies**:\n",
    "     - By running multiple simulations, we can identify strategies that perform well consistently across different random scenarios, rather than relying on a single, potentially lucky outcome.\n",
    "   - **Exploration of Rare Events**:\n",
    "     - Some important outcomes might be rare. Running many simulations increases the chance of capturing these less common but potentially significant scenarios.\n",
    "   - **Uncertainty Quantification**:\n",
    "     - Monte Carlo analysis provides a distribution of outcomes, allowing us to quantify the uncertainty in our predictions and strategies.\n",
    "   - **Sensitivity Analysis**:\n",
    "     - By comparing results across many simulations, we can assess how sensitive our outcomes are to small changes in initial conditions or random factors.\n",
    "   - **Improved Decision Making**:\n",
    "     - The aggregated results from many simulations provide a more comprehensive view of possible outcomes, leading to more informed decision-making.\n",
    "   - **Risk Assessment**:\n",
    "     - The analysis of cost distributions and risk categories across multiple simulations allows for a more thorough assessment of the risks associated with different strategies.\n",
    "   - **Validation of Model Behavior**:\n",
    "     - Running many simulations helps validate that the model behaves as expected across a wide range of scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80740d9f-9200-482a-86a3-083281a587e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function run_single_simulation()\n",
    "    paths = build_graph(initial_modal_shares, final_states)\n",
    "    return sort(paths, by=evaluate_path, rev=true)[1:min(10, length(paths))]\n",
    "end\n",
    "\n",
    "function monte_carlo_analysis(num_simulations::Int)\n",
    "    best_paths = []\n",
    "    for _ in 1:num_simulations\n",
    "        simulation_best_paths = run_single_simulation()\n",
    "        append!(best_paths, simulation_best_paths)\n",
    "    end\n",
    "    return best_paths\n",
    "end\n",
    "\n",
    "function save_best_paths(best_paths, filename::String)\n",
    "    open(filename, \"w\") do io\n",
    "        # Write header\n",
    "        println(io, \"Time,Decisions,Modal_Shares_s,Modal_Shares_c,Modal_Shares_p,Modal_Shares_o,Cost\")\n",
    "        \n",
    "        for path in best_paths\n",
    "            for step in path\n",
    "                decision_names = join([DECISIONS[i] for (i, d) in enumerate(step[\"decisions\"]) if d == 1], \"|\")\n",
    "                modal_shares = step[\"modal_shares\"]\n",
    "                println(io, \"$(step[\"time\"]),$decision_names,$(modal_shares[\"s\"]),$(modal_shares[\"c\"]),$(modal_shares[\"p\"]),$(modal_shares[\"o\"]),$(round(step[\"cost\"], digits=3))\")\n",
    "            end\n",
    "            # Add a blank line between paths\n",
    "            println(io)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function save_analysis_results(action_counts, total_costs, risk_categories, filename::String)\n",
    "    open(filename, \"w\") do io\n",
    "        println(io, \"Category,Item,Value\")\n",
    "        \n",
    "        for (action, count) in action_counts\n",
    "            println(io, \"Action Frequency,$action,$count\")\n",
    "        end\n",
    "        \n",
    "        println(io, \"Cost Analysis,Average Cost,$(mean(total_costs))\")\n",
    "        println(io, \"Cost Analysis,Max Cost,$(maximum(total_costs))\")\n",
    "        println(io, \"Cost Analysis,Min Cost,$(minimum(total_costs))\")\n",
    "        \n",
    "        for (category, count) in risk_categories\n",
    "            println(io, \"Risk Category,$category,$count\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function analyze_best_paths(best_paths)\n",
    "    println(\"Total simulations: $(length(best_paths))\")\n",
    "    \n",
    "    # Initialize data structures for analysis\n",
    "    action_counts = Dict{String, Int}()\n",
    "    total_costs = []\n",
    "    risk_categories = Dict(\"High Risk\" => 0, \"Low Risk\" => 0)\n",
    "    \n",
    "    for path in best_paths\n",
    "        total_cost = 0.0\n",
    "        for step in path\n",
    "            # Count actions\n",
    "            for (i, decision) in enumerate(step[\"decisions\"])\n",
    "                if decision == 1\n",
    "                    action_name = DECISIONS[i]\n",
    "                    action_counts[action_name] = get(action_counts, action_name, 0) + 1\n",
    "                end\n",
    "            end\n",
    "            # Accumulate cost\n",
    "            total_cost += step[\"cost\"]\n",
    "        end\n",
    "        push!(total_costs, total_cost)\n",
    "        \n",
    "        # Categorize path risk\n",
    "        if total_cost > 0.5  # Example threshold for high risk\n",
    "            risk_categories[\"High Risk\"] += 1\n",
    "        else\n",
    "            risk_categories[\"Low Risk\"] += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Print analysis results\n",
    "    println(\"Action Frequencies:\")\n",
    "    for (action, count) in action_counts\n",
    "        println(\"  $action: $count\")\n",
    "    end\n",
    "    \n",
    "    println(\"\\nCost Analysis:\")\n",
    "    println(\"  Average Cost: $(mean(total_costs))\")\n",
    "    println(\"  Max Cost: $(maximum(total_costs))\")\n",
    "    println(\"  Min Cost: $(minimum(total_costs))\")\n",
    "    \n",
    "    println(\"\\nRisk Categories:\")\n",
    "    for (category, count) in risk_categories\n",
    "        println(\"  $category: $count\")\n",
    "    end\n",
    "\n",
    "    # Save analysis results to a CSV file\n",
    "    save_analysis_results(action_counts, total_costs, risk_categories, \"analysis_results.csv\")\n",
    "end\n",
    "\n",
    "function main()\n",
    "    global paths\n",
    "    set_random_seed()  # Remove the fixed seed\n",
    "\n",
    "    num_simulations = 2\n",
    "    best_paths = monte_carlo_analysis(num_simulations)\n",
    "    \n",
    "    # Save best paths to a CSV file\n",
    "    save_best_paths(best_paths, \"best_paths.csv\")\n",
    "    \n",
    "    analyze_best_paths(best_paths)\n",
    "\n",
    "    if isempty(best_paths)\n",
    "        println(\"No feasible paths found.\")\n",
    "        return\n",
    "    end\n",
    "\n",
    "    best_path = find_best_path(best_paths)\n",
    "\n",
    "    if best_path !== nothing\n",
    "        println(\"Best Path:\")\n",
    "        for step in best_path\n",
    "            decision_names = [DECISIONS[i] for (i, d) in enumerate(step[\"decisions\"]) if d == 1]\n",
    "            modal_shares_str = join([k * \": \" * string(round(v, digits=3)) for (k, v) in step[\"modal_shares\"]], \", \")\n",
    "            println(\"Time $(step[\"time\"]):\")\n",
    "            println(\"  Decisions: $(join(decision_names, \", \"))\")\n",
    "            println(\"  Modal Shares: $modal_shares_str\")\n",
    "            println(\"  Cost: $(round(step[\"cost\"], digits=3))\")\n",
    "            println(\"  Debug Log:\")\n",
    "            for log_entry in step[\"debug_log\"]\n",
    "                println(\"    $log_entry\")\n",
    "            end\n",
    "            println()\n",
    "        end\n",
    "        visualize_best_path(best_path)\n",
    "    else\n",
    "        println(\"No feasible paths found.\")\n",
    "    end\n",
    "end\n",
    "\n",
    "if abspath(PROGRAM_FILE) == @__FILE__\n",
    "    main()\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745761c",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "This is a relatively novel approach, I didn't do an extensive literature review but here are some foundation texts for the conceptual approaches used. \n",
    "\n",
    "1. **Puterman, M. L. (1994)**, *Markov Decision Processes: Discrete Stochastic Dynamic Programming*, Wiley.\n",
    "   - This book provides a comprehensive introduction to Markov Decision Processes (MDP), covering both theoretical and practical aspects.\n",
    "\n",
    "2. **Achdou, Y., Cardaliaguet, P., Delarue, F., Porretta, A., & Santambrogio, F. (2020)**, *An Introduction to Mean Field Game Theory*, *Mean Field Games: Cetraro, Italy 2019*, pp. 1-158.\n",
    "   - This paper offers an in-depth introduction to Mean Field Game Theory, explaining how individual decision-making can be studied in the context of large populations.\n",
    "\n",
    "3. **Metropolis, N., & Ulam, S. (1949)**, *The Monte Carlo Method*, *Journal of the American Statistical Association*, 44(247), pp. 335-341.\n",
    "   - The foundational paper that introduces the Monte Carlo Method, a technique for evaluating probabilistic systems through random sampling.\n",
    "\n",
    "4. **Howard, R. A. (1966)**, *Decision Analysis: Applied Decision Theory*, *Proceedings of the Fourth International Conference on Operational Research*.\n",
    "   - A seminal work that lays the foundation for decision analysis, providing a systematic approach for making informed decisions under uncertainty.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
